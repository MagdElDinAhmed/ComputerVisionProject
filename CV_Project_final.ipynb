{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QfEiiSTtnhxL","outputId":"4092fd48-63e5-4e67-ef4d-0e641076452f","executionInfo":{"status":"ok","timestamp":1763978779896,"user_tz":-120,"elapsed":23468,"user":{"displayName":"Abdallah Mohamed El Refaey","userId":"11442150962439552777"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"RfX0jzhApUm_"},"source":["# Image Preprocessing"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"Ac-O3Hres8jk","outputId":"c5e6d2f4-f529-43a1-fe3e-48cbfec669ed","executionInfo":{"status":"ok","timestamp":1763979039086,"user_tz":-120,"elapsed":19568,"user":{"displayName":"Abdallah Mohamed El Refaey","userId":"11442150962439552777"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Processing: 01.jpg\n","  -> Checking for rotation/skew...\n","  -> Rotated by -2.00° (reshape=False, mode=nearest; size preserved).\n","  -> Laplacian Variance (noise sharpness): 1639.90\n","  -> High noise detected (var=1639.90 > 100). Applied Gaussian blur.\n","  -> Contrast Analysis: Darkest Ink Level=88.00, Contrast(StdDev)=29.83\n","  -> Detection: Image is WASHED OUT (Ink is too light: 88.00 > 70)\n","  -> Applying Contrast Stretching & Background Normalization...\n","  -> Sobel Variance (edge sharpness): 10422.21\n","  -> Edges sharp enough (var=10422.21 >= 50). Skipping sharpening.\n","  -> Image Entropy (B&W check): 4.10\n","  -> Image already B&W-like (entropy=4.10 <= 5.0). Skipping binary.\n","  -> Saved enhanced to: /content/drive/MyDrive/Computer Vision/processed_3/01.jpg\n","\n","Processing: 02.jpg\n","  -> Checking for rotation/skew...\n","  -> Rotated by -2.00° (reshape=False, mode=nearest; size preserved).\n","  -> Laplacian Variance (noise sharpness): 1952.76\n","  -> High noise detected (var=1952.76 > 100). Applied Gaussian blur.\n","  -> Contrast Analysis: Darkest Ink Level=74.00, Contrast(StdDev)=35.79\n","  -> Detection: Image is WASHED OUT (Ink is too light: 74.00 > 70)\n","  -> Applying Contrast Stretching & Background Normalization...\n","  -> Sobel Variance (edge sharpness): 10836.98\n","  -> Edges sharp enough (var=10836.98 >= 50). Skipping sharpening.\n","  -> Image Entropy (B&W check): 4.30\n","  -> Image already B&W-like (entropy=4.30 <= 5.0). Skipping binary.\n","  -> Saved enhanced to: /content/drive/MyDrive/Computer Vision/processed_3/02.jpg\n","\n","Processing: 03.jpg\n","  -> Checking for rotation/skew...\n","  -> Laplacian Variance (noise sharpness): 1804.43\n","  -> High noise detected (var=1804.43 > 100). Applied Gaussian blur.\n","  -> Detection: Uneven shadows detected.\n","  -> Contrast Analysis: Darkest Ink Level=81.00, Contrast(StdDev)=39.65\n","  -> Detection: Image is WASHED OUT (Ink is too light: 81.00 > 70)\n","  -> Applying Contrast Stretching & Background Normalization...\n","  -> Sobel Variance (edge sharpness): 12470.84\n","  -> Edges sharp enough (var=12470.84 >= 50). Skipping sharpening.\n","  -> Image Entropy (B&W check): 4.33\n","  -> Image already B&W-like (entropy=4.33 <= 5.0). Skipping binary.\n","  -> Saved enhanced to: /content/drive/MyDrive/Computer Vision/processed_3/03.jpg\n","\n","Processing: 04.jpg\n","  -> Checking for rotation/skew...\n","  -> Starting Iterative Denoising...\n","    -> Testing Kernel 3: Noise Ratio = 0.0034\n","    -> Success! Noise below 0.005 using Kernel 3.\n","  -> Laplacian Variance (noise sharpness): 2452.42\n","  -> High noise detected (var=2452.42 > 100). Applied Gaussian blur.\n","  -> Detection: Uneven shadows detected.\n","  -> Contrast Analysis: Darkest Ink Level=107.00, Contrast(StdDev)=28.56\n","  -> Detection: Image is WASHED OUT (Ink is too light: 107.00 > 70)\n","  -> Applying Contrast Stretching & Background Normalization...\n","  -> Sobel Variance (edge sharpness): 10650.30\n","  -> Edges sharp enough (var=10650.30 >= 50). Skipping sharpening.\n","  -> Image Entropy (B&W check): 2.43\n","  -> Image already B&W-like (entropy=2.43 <= 5.0). Skipping binary.\n","  -> Saved enhanced to: /content/drive/MyDrive/Computer Vision/processed_3/04.jpg\n","\n","Processing: 05.jpg\n","  -> Checking for rotation/skew...\n","  -> Laplacian Variance (noise sharpness): 892.35\n","  -> High noise detected (var=892.35 > 100). Applied Gaussian blur.\n","  -> Contrast Analysis: Darkest Ink Level=112.00, Contrast(StdDev)=25.91\n","  -> Detection: Image is WASHED OUT (Ink is too light: 112.00 > 70)\n","  -> Applying Contrast Stretching & Background Normalization...\n","  -> Sobel Variance (edge sharpness): 8421.19\n","  -> Edges sharp enough (var=8421.19 >= 50). Skipping sharpening.\n","  -> Image Entropy (B&W check): 4.78\n","  -> Image already B&W-like (entropy=4.78 <= 5.0). Skipping binary.\n","  -> Saved enhanced to: /content/drive/MyDrive/Computer Vision/processed_3/05.jpg\n","\n","Processing: 06.jpg\n","  -> Checking for rotation/skew...\n","  -> Rotated by -4.00° (reshape=False, mode=nearest; size preserved).\n","  -> Starting Iterative Denoising...\n","    -> Testing Kernel 3: Noise Ratio = 0.0006\n","    -> Success! Noise below 0.005 using Kernel 3.\n","  -> Laplacian Variance (noise sharpness): 1860.13\n","  -> High noise detected (var=1860.13 > 100). Applied Gaussian blur.\n","  -> Detection: Uneven shadows detected.\n","  -> Contrast Analysis: Darkest Ink Level=41.00, Contrast(StdDev)=43.05\n","  -> Detection: Contrast looks okay.\n","  -> Applying Contrast Stretching & Background Normalization...\n","  -> Sobel Variance (edge sharpness): 14640.68\n","  -> Edges sharp enough (var=14640.68 >= 50). Skipping sharpening.\n","  -> Image Entropy (B&W check): 4.34\n","  -> Image already B&W-like (entropy=4.34 <= 5.0). Skipping binary.\n","  -> Saved enhanced to: /content/drive/MyDrive/Computer Vision/processed_3/06.jpg\n","\n","Processing: 07.jpg\n","  -> Checking for rotation/skew...\n","  -> Rotated by -2.00° (reshape=False, mode=nearest; size preserved).\n","  -> Starting Iterative Denoising...\n","    -> Testing Kernel 3: Noise Ratio = 0.0008\n","    -> Success! Noise below 0.005 using Kernel 3.\n","  -> Laplacian Variance (noise sharpness): 1303.02\n","  -> High noise detected (var=1303.02 > 100). Applied Gaussian blur.\n","  -> Detection: Image is too dark (Avg: 31.65)\n","  -> Contrast Analysis: Darkest Ink Level=2.00, Contrast(StdDev)=16.69\n","  -> Detection: Image is LOW CONTRAST (StdDev: 16.69 < 40)\n","  -> Applying Contrast Stretching & Background Normalization...\n","  -> Sobel Variance (edge sharpness): 11376.18\n","  -> Edges sharp enough (var=11376.18 >= 50). Skipping sharpening.\n","  -> Image Entropy (B&W check): 6.03\n","  -> Applied adaptive binary thresholding (initial density: 0.5399).\n","  -> Checking for interrupted Sudoku lines (optimized repair)...\n","  -> Filtered contours (area>300): 115/352 total.\n","  -> Detected 495 line segments.\n","  -> H fragments: 11, V fragments: 16 (short thr: 37.0).\n","  -> Applied median fallback for noise.\n","  -> Triggered repair (27 > 8). Used kernels H:1x100, V:100x1.\n","  -> Saved enhanced to: /content/drive/MyDrive/Computer Vision/processed_3/07.jpg\n","\n","Processing: 08.jpg\n","  -> Checking for rotation/skew...\n","  -> Laplacian Variance (noise sharpness): 8.74\n","  -> Noise levels okay (var=8.74 <= 100). Skipping blur.\n","  -> Detection: Image is too dark (Avg: 35.21)\n","  -> Contrast Analysis: Darkest Ink Level=32.00, Contrast(StdDev)=1.36\n","  -> Detection: Image is LOW CONTRAST (StdDev: 1.36 < 40)\n","  -> Applying Contrast Stretching & Background Normalization...\n","  -> Sobel Variance (edge sharpness): 7883.33\n","  -> Edges sharp enough (var=7883.33 >= 50). Skipping sharpening.\n","  -> Image Entropy (B&W check): 1.55\n","  -> Image already B&W-like (entropy=1.55 <= 5.0). Skipping binary.\n","  -> Saved enhanced to: /content/drive/MyDrive/Computer Vision/processed_3/08.jpg\n","\n","Processing: 09.jpg\n","  -> Checking for rotation/skew...\n","  -> Laplacian Variance (noise sharpness): 1397.43\n","  -> High noise detected (var=1397.43 > 100). Applied Gaussian blur.\n","  -> Detection: Uneven shadows detected.\n","  -> Contrast Analysis: Darkest Ink Level=65.00, Contrast(StdDev)=31.54\n","  -> Detection: Image is LOW CONTRAST (StdDev: 31.54 < 40)\n","  -> Applying Contrast Stretching & Background Normalization...\n","  -> Sobel Variance (edge sharpness): 12283.75\n","  -> Edges sharp enough (var=12283.75 >= 50). Skipping sharpening.\n","  -> Image Entropy (B&W check): 4.26\n","  -> Image already B&W-like (entropy=4.26 <= 5.0). Skipping binary.\n","  -> Saved enhanced to: /content/drive/MyDrive/Computer Vision/processed_3/09.jpg\n","\n","Processing: 10.jpg\n","  -> Checking for rotation/skew...\n","  -> Rotated by -3.00° (reshape=False, mode=nearest; size preserved).\n","  -> Starting Iterative Denoising...\n","    -> Testing Kernel 3: Noise Ratio = 0.2132\n","    -> Testing Kernel 5: Noise Ratio = 0.0492\n","    -> Testing Kernel 7: Noise Ratio = 0.0103\n","    -> Testing Kernel 9: Noise Ratio = 0.0040\n","    -> Success! Noise below 0.005 using Kernel 9.\n","  -> Laplacian Variance (noise sharpness): 876.41\n","  -> High noise detected (var=876.41 > 100). Applied Gaussian blur.\n","  -> Detection: Uneven shadows detected.\n","  -> Contrast Analysis: Darkest Ink Level=78.00, Contrast(StdDev)=29.11\n","  -> Detection: Image is WASHED OUT (Ink is too light: 78.00 > 70)\n","  -> Applying Contrast Stretching & Background Normalization...\n","  -> Sobel Variance (edge sharpness): 5196.54\n","  -> Edges sharp enough (var=5196.54 >= 50). Skipping sharpening.\n","  -> Image Entropy (B&W check): 5.94\n","  -> Applied adaptive binary thresholding (initial density: 0.5708).\n","  -> Checking for interrupted Sudoku lines (optimized repair)...\n","  -> Filtered contours (area>300): 43/765 total.\n","  -> Detected 770 line segments.\n","  -> H fragments: 40, V fragments: 41 (short thr: 37.0).\n","  -> Applied median fallback for noise.\n","  -> Triggered repair (81 > 8). Used kernels H:1x100, V:100x1.\n","  -> Saved enhanced to: /content/drive/MyDrive/Computer Vision/processed_3/10.jpg\n","\n","Processing: 11.jpg\n","  -> Checking for rotation/skew...\n","  -> Laplacian Variance (noise sharpness): 1084.15\n","  -> High noise detected (var=1084.15 > 100). Applied Gaussian blur.\n","  -> Contrast Analysis: Darkest Ink Level=197.00, Contrast(StdDev)=19.87\n","  -> Detection: Image is WASHED OUT (Ink is too light: 197.00 > 70)\n","  -> Applying Contrast Stretching & Background Normalization...\n","  -> Sobel Variance (edge sharpness): 9860.61\n","  -> Edges sharp enough (var=9860.61 >= 50). Skipping sharpening.\n","  -> Image Entropy (B&W check): 1.86\n","  -> Image already B&W-like (entropy=1.86 <= 5.0). Skipping binary.\n","  -> Saved enhanced to: /content/drive/MyDrive/Computer Vision/processed_3/11.jpg\n","\n","Processing: 12.jpg\n","  -> Checking for rotation/skew...\n","  -> Rotated by 24.00° (reshape=False, mode=nearest; size preserved).\n","  -> Starting Iterative Denoising...\n","    -> Testing Kernel 3: Noise Ratio = 0.0009\n","    -> Success! Noise below 0.005 using Kernel 3.\n","  -> Laplacian Variance (noise sharpness): 385.50\n","  -> High noise detected (var=385.50 > 100). Applied Gaussian blur.\n","  -> Detection: Uneven shadows detected.\n","  -> Contrast Analysis: Darkest Ink Level=105.00, Contrast(StdDev)=27.67\n","  -> Detection: Image is WASHED OUT (Ink is too light: 105.00 > 70)\n","  -> Applying Contrast Stretching & Background Normalization...\n","  -> Sobel Variance (edge sharpness): 7703.86\n","  -> Edges sharp enough (var=7703.86 >= 50). Skipping sharpening.\n","  -> Image Entropy (B&W check): 3.62\n","  -> Image already B&W-like (entropy=3.62 <= 5.0). Skipping binary.\n","  -> Saved enhanced to: /content/drive/MyDrive/Computer Vision/processed_3/12.jpg\n","\n","Processing: 13.jpg\n","  -> Checking for rotation/skew...\n","  -> Starting Iterative Denoising...\n","    -> Testing Kernel 3: Noise Ratio = 0.0020\n","    -> Success! Noise below 0.005 using Kernel 3.\n","  -> Laplacian Variance (noise sharpness): 1675.49\n","  -> High noise detected (var=1675.49 > 100). Applied Gaussian blur.\n","  -> Detection: Uneven shadows detected.\n","  -> Contrast Analysis: Darkest Ink Level=66.00, Contrast(StdDev)=48.02\n","  -> Detection: Contrast looks okay.\n","  -> Applying Contrast Stretching & Background Normalization...\n","  -> Sobel Variance (edge sharpness): 14440.93\n","  -> Edges sharp enough (var=14440.93 >= 50). Skipping sharpening.\n","  -> Image Entropy (B&W check): 5.20\n","  -> Applied adaptive binary thresholding (initial density: 0.6442).\n","  -> Checking for interrupted Sudoku lines (optimized repair)...\n","  -> Filtered contours (area>300): 111/686 total.\n","  -> Detected 1144 line segments.\n","  -> H fragments: 56, V fragments: 37 (short thr: 37.0).\n","  -> Applied median fallback for noise.\n","  -> Triggered repair (93 > 8). Used kernels H:1x100, V:100x1.\n","  -> Saved enhanced to: /content/drive/MyDrive/Computer Vision/processed_3/13.jpg\n","\n","Processing: 14.jpg\n","  -> Checking for rotation/skew...\n","  -> Rotated by -2.00° (reshape=False, mode=nearest; size preserved).\n","  -> Laplacian Variance (noise sharpness): 677.55\n","  -> High noise detected (var=677.55 > 100). Applied Gaussian blur.\n","  -> Detection: Uneven shadows detected.\n","  -> Contrast Analysis: Darkest Ink Level=47.00, Contrast(StdDev)=49.70\n","  -> Detection: Contrast looks okay.\n","  -> Applying Contrast Stretching & Background Normalization...\n","  -> Sobel Variance (edge sharpness): 6866.87\n","  -> Edges sharp enough (var=6866.87 >= 50). Skipping sharpening.\n","  -> Image Entropy (B&W check): 5.01\n","  -> Applied adaptive binary thresholding (initial density: 0.6328).\n","  -> Checking for interrupted Sudoku lines (optimized repair)...\n","  -> Filtered contours (area>300): 41/210 total.\n","  -> Detected 334 line segments.\n","  -> H fragments: 5, V fragments: 16 (short thr: 37.0).\n","  -> Applied median fallback for noise.\n","  -> Triggered repair (21 > 8). Used kernels H:1x100, V:100x1.\n","  -> Saved enhanced to: /content/drive/MyDrive/Computer Vision/processed_3/14.jpg\n","\n","Processing: 15.jpg\n","  -> Checking for rotation/skew...\n","  -> Laplacian Variance (noise sharpness): 994.25\n","  -> High noise detected (var=994.25 > 100). Applied Gaussian blur.\n","  -> Contrast Analysis: Darkest Ink Level=142.00, Contrast(StdDev)=25.93\n","  -> Detection: Image is WASHED OUT (Ink is too light: 142.00 > 70)\n","  -> Applying Contrast Stretching & Background Normalization...\n","  -> Sobel Variance (edge sharpness): 9012.01\n","  -> Edges sharp enough (var=9012.01 >= 50). Skipping sharpening.\n","  -> Image Entropy (B&W check): 3.56\n","  -> Image already B&W-like (entropy=3.56 <= 5.0). Skipping binary.\n","  -> Saved enhanced to: /content/drive/MyDrive/Computer Vision/processed_3/15.jpg\n","\n","Processing: 16.jpg\n","  -> Checking for rotation/skew...\n","  -> Rotated by -12.50° (reshape=False, mode=nearest; size preserved).\n","  -> Starting Iterative Denoising...\n","    -> Testing Kernel 3: Noise Ratio = 0.0055\n","    -> Testing Kernel 5: Noise Ratio = 0.0014\n","    -> Success! Noise below 0.005 using Kernel 5.\n","  -> Laplacian Variance (noise sharpness): 429.63\n","  -> High noise detected (var=429.63 > 100). Applied Gaussian blur.\n","  -> Contrast Analysis: Darkest Ink Level=242.00, Contrast(StdDev)=9.62\n","  -> Detection: Image is WASHED OUT (Ink is too light: 242.00 > 70)\n","  -> Applying Contrast Stretching & Background Normalization...\n","  -> Sobel Variance (edge sharpness): 13673.48\n","  -> Edges sharp enough (var=13673.48 >= 50). Skipping sharpening.\n","  -> Image Entropy (B&W check): 1.76\n","  -> Image already B&W-like (entropy=1.76 <= 5.0). Skipping binary.\n","  -> Saved enhanced to: /content/drive/MyDrive/Computer Vision/processed_3/16.jpg\n","\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 1500x8000 with 0 Axes>"]},"metadata":{}}],"source":["import cv2\n","import numpy as np\n","import os\n","import matplotlib.pyplot as plt\n","\n","# === Your Exact: Needs Rotation Detection (Unchanged) ===\n","\n","def needs_rotation(gray):\n","    edges = cv2.Canny(gray, 50, 150)\n","    lines = cv2.HoughLines(edges, 1, np.pi/180, 150)\n","    if lines is None:\n","        return False, 0\n","    angles = []\n","    for line in lines:\n","        for rho, theta in line:\n","            angle = (theta*180)/np.pi\n","            if 20 < angle < 160:\n","                angles.append(angle)\n","    if angles:\n","        main_angle = np.median(angles)\n","        rot_angle = main_angle - 90 if main_angle > 45 else main_angle\n","        return abs(rot_angle) > 2, rot_angle\n","    return False, 0\n","\n","# === Rotate Function (Unchanged) ===\n","\n","def rotate(gray, angle, reshape=False, mode='nearest'):\n","    h, w = gray.shape\n","    center = (w // 2, h // 2)\n","    M = cv2.getRotationMatrix2D(center, angle, 1.0)\n","    flags = cv2.INTER_NEAREST if mode == 'nearest' else cv2.INTER_LINEAR\n","    rotated = cv2.warpAffine(gray, M, (w, h), flags=flags, borderMode=cv2.BORDER_REPLICATE)\n","    print(f\"  -> Rotated by {angle:.2f}° (reshape={reshape}, mode={mode}; size preserved).\")\n","    return rotated\n","\n","# === 1. Detection: Washed Out / Faded Areas (Unchanged) ===\n","\n","def check_for_washout(img_gray, ink_threshold=70, contrast_threshold=40):\n","    darkest_pixels_val = np.percentile(img_gray, 5)\n","    std_dev = np.std(img_gray)\n","    print(f\"  -> Contrast Analysis: Darkest Ink Level={darkest_pixels_val:.2f}, Contrast(StdDev)={std_dev:.2f}\")\n","    if darkest_pixels_val > ink_threshold:\n","        print(f\"  -> Detection: Image is WASHED OUT (Ink is too light: {darkest_pixels_val:.2f} > {ink_threshold})\")\n","        return True\n","    if std_dev < contrast_threshold:\n","        print(f\"  -> Detection: Image is LOW CONTRAST (StdDev: {std_dev:.2f} < {contrast_threshold})\")\n","        return True\n","    print(\"  -> Detection: Contrast looks okay.\")\n","    return False\n","\n","# === 2. Noise Measurement Helper (Unchanged) ===\n","\n","def get_noise_ratio(img_gray, noise_threshold=10):\n","    denoised_ref = cv2.medianBlur(img_gray, 3)\n","    diff = cv2.absdiff(img_gray, denoised_ref)\n","    noise_mask = diff > noise_threshold\n","    noise_pixel_count = np.sum(noise_mask)\n","    return noise_pixel_count / img_gray.size\n","\n","# === Laplacian Variance (Unchanged) ===\n","def get_laplacian_variance(img_gray, blur_size=3):\n","    laplacian = cv2.Laplacian(img_gray, cv2.CV_64F, ksize=blur_size)\n","    variance = laplacian.var()\n","    print(f\"  -> Laplacian Variance (noise sharpness): {variance:.2f}\")\n","    return variance\n","\n","# === Apply Gaussian Blur (Unchanged) ===\n","def apply_gaussian_blur_if_needed(img_gray, variance_threshold=100):\n","    variance = get_laplacian_variance(img_gray)\n","    if variance > variance_threshold:\n","        blurred = cv2.GaussianBlur(img_gray, (3, 3), sigmaX=0.5)\n","        print(f\"  -> High noise detected (var={variance:.2f} > {variance_threshold}). Applied Gaussian blur.\")\n","        return blurred, True\n","    else:\n","        print(f\"  -> Noise levels okay (var={variance:.2f} <= {variance_threshold}). Skipping blur.\")\n","        return img_gray, False\n","\n","# === Sobel Variance (Unchanged) ===\n","def get_sobel_variance(img_gray, ksize=3):\n","    sobelx = cv2.Sobel(img_gray, cv2.CV_64F, 1, 0, ksize=ksize)\n","    sobely = cv2.Sobel(img_gray, cv2.CV_64F, 0, 1, ksize=ksize)\n","    sobel_combined = np.sqrt(sobelx**2 + sobely**2)\n","    variance = sobel_combined.var()\n","    print(f\"  -> Sobel Variance (edge sharpness): {variance:.2f}\")\n","    return variance\n","\n","# === Apply Unsharp Sharpen (Unchanged) ===\n","def apply_unsharp_sharpen_if_needed(img_gray, variance_threshold=50, amount=1.0):\n","    variance = get_sobel_variance(img_gray)\n","    if variance < variance_threshold:\n","        blurred = cv2.GaussianBlur(img_gray, (5, 5), sigmaX=1.0)\n","        sharpened = cv2.addWeighted(img_gray, 1.5, blurred, -0.5, 0)\n","        print(f\"  -> Soft edges detected (var={variance:.2f} < {variance_threshold}). Applied unsharp sharpening (amount={amount}).\")\n","        return sharpened, True\n","    else:\n","        print(f\"  -> Edges sharp enough (var={variance:.2f} >= {variance_threshold}). Skipping sharpening.\")\n","        return img_gray, False\n","\n","# === Binary Thresholding (Unchanged) ===\n","def apply_binary_thresholding_if_needed(img_gray, entropy_threshold=5.0, block_size=11, C=2):\n","    hist = cv2.calcHist([img_gray], [0], None, [256], [0, 256])\n","    entropy = -np.sum((hist / hist.sum()) * np.log2(hist / hist.sum() + 1e-10))\n","    print(f\"  -> Image Entropy (B&W check): {entropy:.2f}\")\n","\n","    if entropy > entropy_threshold:\n","        binary = cv2.adaptiveThreshold(img_gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n","                                       cv2.THRESH_BINARY_INV, block_size, C)\n","        dark_pixels = np.sum(binary == 0)\n","        total_pixels = binary.size\n","        density = dark_pixels / total_pixels\n","        if density < 0.05 or density > 0.5:\n","            _, binary = cv2.threshold(img_gray, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n","        print(f\"  -> Applied adaptive binary thresholding (initial density: {density:.4f}).\")\n","        return binary, True\n","    else:\n","        print(f\"  -> Image already B&W-like (entropy={entropy:.2f} <= {entropy_threshold}). Skipping binary.\")\n","        return img_gray, False\n","\n","# === Optimized: Repair Lines (Unchanged from Prior) ===\n","\n","def repair_sudoku_lines_if_needed(binary_img, fragment_threshold=8, min_length=30):\n","    print(\"  -> Checking for interrupted Sudoku lines (optimized repair)...\")\n","\n","    contours, _ = cv2.findContours(binary_img, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n","    line_mask = np.zeros_like(binary_img)\n","    large_contours_count = 0\n","    for cnt in contours:\n","        area = cv2.contourArea(cnt)\n","        if area > 300:\n","            cv2.drawContours(line_mask, [cnt], -1, 255, -1)\n","            large_contours_count += 1\n","    binary_lines = cv2.bitwise_and(binary_img, line_mask)\n","    print(f\"  -> Filtered contours (area>300): {large_contours_count}/{len(contours)} total.\")\n","\n","    dil_kernel = np.ones((3, 3), np.uint8)\n","    binary_lines_dil = cv2.dilate(binary_lines, dil_kernel, iterations=1)\n","\n","    edges = cv2.Canny(binary_lines_dil, 50, 150, apertureSize=3)\n","    lines = cv2.HoughLinesP(edges, rho=1, theta=np.pi/180, threshold=25,\n","                            minLineLength=min_length, maxLineGap=30)\n","\n","    if lines is None:\n","        lines = np.array([])\n","        print(\"  -> No lines detected by Hough.\")\n","\n","    print(f\"  -> Detected {len(lines)} line segments.\")\n","\n","    fragments = 0\n","    img_h, img_w = binary_img.shape\n","    expected_len = max(img_h, img_w) / 9\n","    short_threshold = expected_len / 3\n","    h_fragments = 0\n","    v_fragments = 0\n","    for line in lines:\n","        x1, y1, x2, y2 = line[0]\n","        length = np.sqrt((x2 - x1)**2 + (y2 - y1)**2)\n","        if abs(x2 - x1) >= abs(y2 - y1) and abs(x2 - x1) > 15:\n","            if length < short_threshold:\n","                h_fragments += 1\n","        elif abs(y2 - y1) >= abs(x2 - x1) and abs(y2 - y1) > 15:\n","            if length < short_threshold:\n","                v_fragments += 1\n","    fragments = h_fragments + v_fragments\n","    print(f\"  -> H fragments: {h_fragments}, V fragments: {v_fragments} (short thr: {short_threshold:.1f}).\")\n","\n","    if fragments > fragment_threshold:\n","        h_kernel_size = max(1, img_w // 10)\n","        v_kernel_size = max(1, img_h // 10)\n","        h_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (1, h_kernel_size))\n","        repaired_h = cv2.morphologyEx(binary_lines, cv2.MORPH_CLOSE, h_kernel, iterations=3)\n","\n","        v_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (v_kernel_size, 1))\n","        repaired_v = cv2.morphologyEx(repaired_h, cv2.MORPH_CLOSE, v_kernel, iterations=3)\n","\n","        open_kernel = np.ones((3, 3), np.uint8)\n","        repaired = cv2.morphologyEx(repaired_v, cv2.MORPH_OPEN, open_kernel, iterations=2)\n","        repaired = cv2.dilate(repaired, np.ones((3, 3), np.uint8), iterations=2)\n","\n","        if np.sum(repaired > 200) / repaired.size > 0.015:\n","            repaired = cv2.medianBlur(repaired, 3)\n","            print(\"  -> Applied median fallback for noise.\")\n","\n","        repaired = cv2.bitwise_or(repaired, binary_img)\n","\n","        print(f\"  -> Triggered repair ({fragments} > {fragment_threshold}). Used kernels H:1x{h_kernel_size}, V:{v_kernel_size}x1.\")\n","        return repaired, True\n","    else:\n","        print(f\"  -> Low fragments ({fragments} <= {fragment_threshold}). Skipping repair.\")\n","        return binary_img, False\n","\n","# === New: Extract Sudoku Board (Adapted from Reference) ===\n","\n","def extract_sudoku_board(img_color, img_gray, img_binary=None, epsilon=0.015):\n","    \"\"\"\n","    Adapted: Detects largest quad contour on binary/gray; orders corners; warps to top-down rectangle.\n","    Uses reference logic: medianBlur + adaptive thresh; max dist dims; clockwise order.\n","    Returns warped color/gray/binary (if provided); skips if no 4 points.\n","    \"\"\"\n","    print(\"  -> Extracting Sudoku board (perspective warp)...\")\n","\n","    # Prep: Blur + adaptive on gray (as reference)\n","    blur = cv2.medianBlur(img_gray, 3)\n","    thresh = cv2.adaptiveThreshold(blur, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n","                                   cv2.THRESH_BINARY_INV, 11, 3)\n","\n","    # Contours: Largest external\n","    cnts = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n","    cnts = cnts[0] if len(cnts) == 2 else cnts[1]\n","    cnts = sorted(cnts, key=cv2.contourArea, reverse=True)\n","\n","    if not cnts:\n","        print(\"  -> No contours found. Skipping board extraction.\")\n","        return img_color, img_gray, img_binary or img_gray, False\n","\n","    # Largest contour approx\n","    c = cnts[0]\n","    peri = cv2.arcLength(c, True)\n","    approx = cv2.approxPolyDP(c, epsilon * peri, True)\n","\n","    if len(approx) != 4:\n","        print(f\"  -> Largest contour approx has {len(approx)} points (need 4). Skipping board extraction.\")\n","        return img_color, img_gray, img_binary or img_gray, False\n","\n","    # Order corners (reference: clockwise TL, TR, BR, BL via sums/diffs)\n","    corners = approx.reshape(4, 2)\n","    s = corners.sum(axis=1)\n","    diff = np.diff(corners, axis=1)\n","    top_l = corners[np.argmin(s)]\n","    top_r = corners[np.argmin(diff)]\n","    bottom_r = corners[np.argmax(s)]\n","    bottom_l = corners[np.argmax(diff)]\n","    ordered_corners = np.array([top_l, top_r, bottom_r, bottom_l], dtype=\"float32\")\n","\n","    # Dimensions (max width/height from distances)\n","    width_a = np.sqrt(((bottom_r[0] - bottom_l[0]) ** 2) + ((bottom_r[1] - bottom_l[1]) ** 2))\n","    width_b = np.sqrt(((top_r[0] - top_l[0]) ** 2) + ((top_r[1] - top_l[1]) ** 2))\n","    width = max(int(width_a), int(width_b))\n","\n","    height_a = np.sqrt(((top_r[0] - bottom_r[0]) ** 2) + ((top_r[1] - bottom_r[1]) ** 2))\n","    height_b = np.sqrt(((top_l[0] - bottom_l[0]) ** 2) + ((top_l[1] - bottom_l[1]) ** 2))\n","    height = max(int(height_a), int(height_b))\n","\n","    # Dst points: TL, TR, BR, BL\n","    dimensions = np.array([[0, 0], [width - 1, 0], [width - 1, height - 1], [0, height - 1]], dtype=\"float32\")\n","\n","    # Transform matrix\n","    matrix = cv2.getPerspectiveTransform(ordered_corners, dimensions)\n","\n","    # Warp color, gray, binary (if avail)\n","    warped_color = cv2.warpPerspective(img_color, matrix, (width, height))\n","    warped_gray = cv2.warpPerspective(img_gray, matrix, (width, height))\n","    if img_binary is not None:\n","        warped_binary = cv2.warpPerspective(img_binary, matrix, (width, height))\n","    else:\n","        warped_binary = warped_gray\n","\n","    print(f\"  -> Board extracted: {warped_color.shape[:2]} (w:{width}, h:{height}).\")\n","    return warped_color, warped_gray, warped_binary, True\n","\n","# === 3. Iterative Denoising (Unchanged) ===\n","\n","def iterative_denoise(img_gray, max_kernel=19, target_ratio=0.01):\n","    kernel = 3\n","    best_img = img_gray.copy()\n","    print(\"  -> Starting Iterative Denoising...\")\n","\n","    while kernel <= max_kernel:\n","        temp_img = cv2.medianBlur(img_gray, kernel)\n","        current_ratio = get_noise_ratio(temp_img)\n","        print(f\"    -> Testing Kernel {kernel}: Noise Ratio = {current_ratio:.4f}\")\n","\n","        if current_ratio < target_ratio:\n","            print(f\"    -> Success! Noise below {target_ratio} using Kernel {kernel}.\")\n","            return temp_img, kernel\n","\n","        best_img = temp_img\n","        kernel += 2\n","\n","    print(f\"    -> Warning: Reached Max Kernel ({max_kernel}) without fully cleaning. Using result anyway.\")\n","    return best_img, max_kernel\n","\n","# === 4. Lighting Fix (Unchanged) ===\n","\n","def check_if_lighting_fix_needed(img_gray, dark_threshold=110, shadow_variance=60):\n","    avg_brightness = np.mean(img_gray)\n","    if avg_brightness < dark_threshold:\n","        print(f\"  -> Detection: Image is too dark (Avg: {avg_brightness:.2f})\")\n","        return True\n","\n","    thumbnail = cv2.resize(img_gray, (20, 20))\n","    min_val = np.min(thumbnail)\n","    max_val = np.max(thumbnail)\n","\n","    if (max_val - min_val) > (255 - dark_threshold) and min_val < shadow_variance:\n","        print(\"  -> Detection: Uneven shadows detected.\")\n","        return True\n","    return False\n","\n","def fix_lighting_and_shadows(img_gray):\n","    print(\"  -> Applying Contrast Stretching & Background Normalization...\")\n","    dilated = cv2.dilate(img_gray, np.ones((7, 7), np.uint8))\n","    bg_img = cv2.medianBlur(dilated, 21)\n","    diff_img = 255 - cv2.absdiff(img_gray, bg_img)\n","    norm_img = diff_img.copy()\n","    cv2.normalize(diff_img, norm_img, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_8UC1)\n","    clahe = cv2.createCLAHE(clipLimit=3.8, tileGridSize=(10,10))\n","    return clahe.apply(norm_img)\n","\n","# === 5. Updated Pipeline (With Board Extraction as Final Step) ===\n","\n","def process_image_pipeline(image_path, output_folder):\n","    img_color = cv2.imread(image_path)\n","    if img_color is None:\n","        print(f\"Error: Could not read {image_path}\")\n","        return\n","\n","    current_gray = cv2.cvtColor(img_color, cv2.COLOR_BGR2GRAY)\n","    actions_taken = []\n","\n","    # --- STEP 0: ROTATION ---\n","    print(\"  -> Checking for rotation/skew...\")\n","    rotated, angle = needs_rotation(current_gray)\n","    if rotated:\n","        current_gray = rotate(current_gray, angle, reshape=False, mode='nearest')\n","        img_color = cv2.cvtColor(current_gray, cv2.COLOR_GRAY2BGR)  # Update color too\n","        actions_taken.append(\"rotated\")\n","\n","    # --- STEP A: NOISE & BLUR ---\n","    initial_noise = get_noise_ratio(current_gray)\n","    if initial_noise > 0.01:\n","        current_gray, final_k = iterative_denoise(current_gray, max_kernel=19, target_ratio=0.005)\n","        img_color = cv2.cvtColor(current_gray, cv2.COLOR_GRAY2BGR)\n","        actions_taken.append(f\"denoisedK{final_k}\")\n","\n","    current_gray, blurred_flag = apply_gaussian_blur_if_needed(current_gray, variance_threshold=100)\n","    if blurred_flag:\n","        img_color = cv2.cvtColor(current_gray, cv2.COLOR_GRAY2BGR)\n","        actions_taken.append(\"gaussian_blur\")\n","\n","    # --- STEP B: LIGHTING & WASHOUT ---\n","    is_dark_or_shadowed = check_if_lighting_fix_needed(current_gray)\n","    is_washed_out = check_for_washout(current_gray)\n","    if is_dark_or_shadowed or is_washed_out:\n","        current_gray = fix_lighting_and_shadows(current_gray)\n","        img_color = cv2.cvtColor(current_gray, cv2.COLOR_GRAY2BGR)\n","        actions_taken.append(\"fixed_contrast\")\n","\n","    # --- STEP D: SHARPENING ---\n","    current_gray, sharpen_flag = apply_unsharp_sharpen_if_needed(current_gray, variance_threshold=50)\n","    if sharpen_flag:\n","        img_color = cv2.cvtColor(current_gray, cv2.COLOR_GRAY2BGR)\n","        actions_taken.append(\"sharpened\")\n","\n","    # --- STEP E: BINARY & REPAIR ---\n","    binary_img, binary_flag = apply_binary_thresholding_if_needed(current_gray, entropy_threshold=5.0)\n","    if binary_flag:\n","        actions_taken.append(\"binary\")\n","        repaired_img, repair_flag = repair_sudoku_lines_if_needed(binary_img, fragment_threshold=8)\n","        if repair_flag:\n","            binary_img = repaired_img\n","            actions_taken.append(\"lines_repaired\")\n","\n","    # --- STEP G: SAVE (Enhanced + Binary + Board) ---\n","\n","    if not os.path.exists(output_folder):\n","        os.makedirs(output_folder)\n","\n","    filename = os.path.basename(image_path)\n","    enhanced_path = os.path.join(output_folder, f\"{filename}\")\n","    cv2.imwrite(enhanced_path, current_gray)\n","\n","    print(f\"  -> Saved enhanced to: {enhanced_path}\\n\")\n","\n","# --- RUNNER (Updated for 16.jpg) ---\n","\n","input_files = [\n","    '/content/drive/MyDrive/Computer Vision/input/01.jpg',\n","    '/content/drive/MyDrive/Computer Vision/input/02.jpg',\n","    '/content/drive/MyDrive/Computer Vision/input/03.jpg',\n","    '/content/drive/MyDrive/Computer Vision/input/04.jpg',\n","    '/content/drive/MyDrive/Computer Vision/input/05.jpg',\n","    '/content/drive/MyDrive/Computer Vision/input/06.jpg',\n","    '/content/drive/MyDrive/Computer Vision/input/07.jpg',\n","    '/content/drive/MyDrive/Computer Vision/input/08.jpg',\n","    '/content/drive/MyDrive/Computer Vision/input/09.jpg',\n","    '/content/drive/MyDrive/Computer Vision/input/10.jpg',\n","    '/content/drive/MyDrive/Computer Vision/input/11.jpg',\n","    '/content/drive/MyDrive/Computer Vision/input/12.jpg',\n","    '/content/drive/MyDrive/Computer Vision/input/13.jpg',\n","    '/content/drive/MyDrive/Computer Vision/input/14.jpg',\n","    '/content/drive/MyDrive/Computer Vision/input/15.jpg',\n","    '/content/drive/MyDrive/Computer Vision/input/16.jpg'\n","]\n","\n","output_dir = \"/content/drive/MyDrive/Computer Vision/processed_3/\"\n","\n","for file_path in input_files:\n","    if os.path.exists(file_path):\n","        print(f\"Processing: {os.path.basename(file_path)}\")\n","        process_image_pipeline(file_path, output_dir)\n","    else:\n","        print(f\"File not found: {file_path}\")\n","\n","# Updated Helper (Added lines_repaired variants)\n","def show_processed_images(folder, file_list, actions=[\"original_\", \"denoised_\", \"gaussian_blur_\", \"fixed_contrast_\", \"denoisedK5_fixed_contrast_gaussian_blur_\", \"sharpened_\", \"binary_\", \"lines_repaired_\"]):\n","    plt.figure(figsize=(15, 5 * len(file_list)))\n","    img_id = 1\n","    for fname in file_list:\n","        for action in actions:\n","            full_path = os.path.join(folder, f\"{action}{os.path.basename(fname)}\")\n","            if os.path.exists(full_path):\n","                img = cv2.imread(full_path, cv2.IMREAD_GRAYSCALE)\n","                plt.subplot(len(file_list), len(actions), img_id)\n","                plt.imshow(img, cmap='gray')\n","                plt.title(f\"{os.path.basename(fname)}\\n[{action[:-1]}]\" if action != \"original_\" else f\"{os.path.basename(fname)}\\n[Original]\")\n","                plt.axis('off')\n","                img_id += 1\n","\n","# Call this after processing\n","show_processed_images(output_dir, input_files)\n","plt.tight_layout()\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"yLGl6YiwqDeX"},"source":["# Detect board frame"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EwYBCr7jnyhP","outputId":"03250342-19ce-41e9-adce-4c94735795ee","executionInfo":{"status":"ok","timestamp":1763979049254,"user_tz":-120,"elapsed":2450,"user":{"displayName":"Abdallah Mohamed El Refaey","userId":"11442150962439552777"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","============================================================\n","Processing: 01.jpg\n","============================================================\n","Detected 157 line segments\n","✓ Saved Hough lines: /content/drive/MyDrive/Computer Vision/warped_batch/01_hough_lines.jpg\n","Corners detected: [[37.0, 45.0], [960.0, 45.0], [960.0, 952.0], [37.0, 952.0]]\n","✓ Saved detected corners: /content/drive/MyDrive/Computer Vision/warped_batch/01_corners.jpg\n","✓ Perspective transform successful! Output size: (923, 923)\n","✓ Processing complete for 01.jpg\n","\n","\n","============================================================\n","Processing: 02.jpg\n","============================================================\n","Detected 181 line segments\n","✓ Saved Hough lines: /content/drive/MyDrive/Computer Vision/warped_batch/02_hough_lines.jpg\n","Corners detected: [[34.0, 55.0], [959.0, 55.0], [959.0, 948.0], [34.0, 948.0]]\n","✓ Saved detected corners: /content/drive/MyDrive/Computer Vision/warped_batch/02_corners.jpg\n","✓ Perspective transform successful! Output size: (925, 925)\n","✓ Processing complete for 02.jpg\n","\n","\n","============================================================\n","Processing: 03.jpg\n","============================================================\n","Detected 126 line segments\n","✓ Saved Hough lines: /content/drive/MyDrive/Computer Vision/warped_batch/03_hough_lines.jpg\n","Corners detected: [[43.0, 7.0], [951.0, 7.0], [951.0, 931.0], [43.0, 931.0]]\n","✓ Saved detected corners: /content/drive/MyDrive/Computer Vision/warped_batch/03_corners.jpg\n","✓ Perspective transform successful! Output size: (924, 924)\n","✓ Processing complete for 03.jpg\n","\n","\n","============================================================\n","Processing: 04.jpg\n","============================================================\n","Detected 134 line segments\n","✓ Saved Hough lines: /content/drive/MyDrive/Computer Vision/warped_batch/04_hough_lines.jpg\n","Corners detected: [[48.0, 40.0], [973.0, 40.0], [973.0, 959.0], [48.0, 959.0]]\n","✓ Saved detected corners: /content/drive/MyDrive/Computer Vision/warped_batch/04_corners.jpg\n","✓ Perspective transform successful! Output size: (925, 925)\n","✓ Processing complete for 04.jpg\n","\n","\n","============================================================\n","Processing: 05.jpg\n","============================================================\n","Detected 138 line segments\n","✓ Saved Hough lines: /content/drive/MyDrive/Computer Vision/warped_batch/05_hough_lines.jpg\n","Corners detected: [[59.0, 76.0], [932.0, 76.0], [932.0, 935.0], [59.0, 935.0]]\n","✓ Saved detected corners: /content/drive/MyDrive/Computer Vision/warped_batch/05_corners.jpg\n","✓ Perspective transform successful! Output size: (873, 873)\n","✓ Processing complete for 05.jpg\n","\n","\n","============================================================\n","Processing: 06.jpg\n","============================================================\n","Detected 133 line segments\n","✓ Saved Hough lines: /content/drive/MyDrive/Computer Vision/warped_batch/06_hough_lines.jpg\n","Corners detected: [[97.0, 75.0], [971.0, 75.0], [971.0, 909.0], [97.0, 909.0]]\n","✓ Saved detected corners: /content/drive/MyDrive/Computer Vision/warped_batch/06_corners.jpg\n","✓ Perspective transform successful! Output size: (874, 874)\n","✓ Processing complete for 06.jpg\n","\n","\n","============================================================\n","Processing: 07.jpg\n","============================================================\n","Detected 150 line segments\n","✓ Saved Hough lines: /content/drive/MyDrive/Computer Vision/warped_batch/07_hough_lines.jpg\n","Corners detected: [[95.0, 63.0], [962.0, 63.0], [962.0, 966.0], [95.0, 966.0]]\n","✓ Saved detected corners: /content/drive/MyDrive/Computer Vision/warped_batch/07_corners.jpg\n","✓ Perspective transform successful! Output size: (903, 903)\n","✓ Processing complete for 07.jpg\n","\n","\n","============================================================\n","Processing: 08.jpg\n","============================================================\n","Detected 163 line segments\n","✓ Saved Hough lines: /content/drive/MyDrive/Computer Vision/warped_batch/08_hough_lines.jpg\n","Corners detected: [[47.0, 36.0], [943.0, 36.0], [943.0, 955.0], [47.0, 955.0]]\n","✓ Saved detected corners: /content/drive/MyDrive/Computer Vision/warped_batch/08_corners.jpg\n","✓ Perspective transform successful! Output size: (919, 919)\n","✓ Processing complete for 08.jpg\n","\n","\n","============================================================\n","Processing: 09.jpg\n","============================================================\n","Detected 165 line segments\n","✓ Saved Hough lines: /content/drive/MyDrive/Computer Vision/warped_batch/09_hough_lines.jpg\n","Corners detected: [[47.0, 42.0], [965.0, 42.0], [965.0, 935.0], [47.0, 935.0]]\n","✓ Saved detected corners: /content/drive/MyDrive/Computer Vision/warped_batch/09_corners.jpg\n","✓ Perspective transform successful! Output size: (918, 918)\n","✓ Processing complete for 09.jpg\n","\n","\n","============================================================\n","Processing: 10.jpg\n","============================================================\n","Detected 88 line segments\n","✓ Saved Hough lines: /content/drive/MyDrive/Computer Vision/warped_batch/10_hough_lines.jpg\n","Corners detected: [[41.0, 21.0], [971.0, 21.0], [971.0, 921.0], [41.0, 921.0]]\n","✓ Saved detected corners: /content/drive/MyDrive/Computer Vision/warped_batch/10_corners.jpg\n","✓ Perspective transform successful! Output size: (930, 930)\n","✓ Processing complete for 10.jpg\n","\n","\n","============================================================\n","Processing: 11.jpg\n","============================================================\n","Detected 87 line segments\n","✓ Saved Hough lines: /content/drive/MyDrive/Computer Vision/warped_batch/11_hough_lines.jpg\n","Corners detected: [[32.0, 16.0], [984.0, 16.0], [984.0, 966.0], [32.0, 966.0]]\n","✓ Saved detected corners: /content/drive/MyDrive/Computer Vision/warped_batch/11_corners.jpg\n","✓ Perspective transform successful! Output size: (952, 952)\n","✓ Processing complete for 11.jpg\n","\n","\n","============================================================\n","Processing: 12.jpg\n","============================================================\n","Detected 222 line segments\n","✓ Saved Hough lines: /content/drive/MyDrive/Computer Vision/warped_batch/12_hough_lines.jpg\n","Corners detected: [[155.0, 163.0], [1099.0, 163.0], [1099.0, 1101.0], [155.0, 1101.0]]\n","✓ Saved detected corners: /content/drive/MyDrive/Computer Vision/warped_batch/12_corners.jpg\n","✓ Perspective transform successful! Output size: (944, 944)\n","✓ Processing complete for 12.jpg\n","\n","\n","============================================================\n","Processing: 13.jpg\n","============================================================\n","Detected 218 line segments\n","✓ Saved Hough lines: /content/drive/MyDrive/Computer Vision/warped_batch/13_hough_lines.jpg\n","Corners detected: [[42.0, 37.0], [974.0, 37.0], [974.0, 954.0], [42.0, 954.0]]\n","✓ Saved detected corners: /content/drive/MyDrive/Computer Vision/warped_batch/13_corners.jpg\n","✓ Perspective transform successful! Output size: (932, 932)\n","✓ Processing complete for 13.jpg\n","\n","\n","============================================================\n","Processing: 14.jpg\n","============================================================\n","Detected 153 line segments\n","✓ Saved Hough lines: /content/drive/MyDrive/Computer Vision/warped_batch/14_hough_lines.jpg\n","Corners detected: [[76.0, 40.0], [978.0, 40.0], [978.0, 943.0], [76.0, 943.0]]\n","✓ Saved detected corners: /content/drive/MyDrive/Computer Vision/warped_batch/14_corners.jpg\n","✓ Perspective transform successful! Output size: (903, 903)\n","✓ Processing complete for 14.jpg\n","\n","\n","============================================================\n","Processing: 15.jpg\n","============================================================\n","Detected 152 line segments\n","✓ Saved Hough lines: /content/drive/MyDrive/Computer Vision/warped_batch/15_hough_lines.jpg\n","Corners detected: [[41.0, 45.0], [966.0, 45.0], [966.0, 984.0], [41.0, 984.0]]\n","✓ Saved detected corners: /content/drive/MyDrive/Computer Vision/warped_batch/15_corners.jpg\n","✓ Perspective transform successful! Output size: (939, 939)\n","✓ Processing complete for 15.jpg\n","\n","\n","============================================================\n","Processing: 16.jpg\n","============================================================\n","Detected 82 line segments\n","✓ Saved Hough lines: /content/drive/MyDrive/Computer Vision/warped_batch/16_hough_lines.jpg\n","Corners detected: [[144.0, 136.0], [1052.0, 136.0], [1052.0, 1042.0], [144.0, 1042.0]]\n","✓ Saved detected corners: /content/drive/MyDrive/Computer Vision/warped_batch/16_corners.jpg\n","✓ Perspective transform successful! Output size: (908, 908)\n","✓ Processing complete for 16.jpg\n","\n","\n","============================================================\n","BATCH PROCESSING COMPLETE\n","============================================================\n","Total images: 16\n","Successful: 16\n","Failed: 0\n"]}],"source":["import cv2\n","import numpy as np\n","import os\n","from pathlib import Path\n","\n","\n","def process_sudoku_image(input_path, output_folder, save_intermediates=False):\n","    \"\"\"\n","    Process a Sudoku image: detect grid, apply perspective transform, and save result.\n","\n","    Parameters:\n","    -----------\n","    input_path : str\n","        Path to input Sudoku image\n","    output_folder : str\n","        Folder to save processed images\n","    save_intermediates : bool\n","        Whether to save intermediate processing steps (default: False)\n","\n","    Returns:\n","    --------\n","    bool : True if successful, False otherwise\n","    \"\"\"\n","\n","    # Create output folder if it doesn't exist\n","    os.makedirs(output_folder, exist_ok=True)\n","\n","    # Get input filename\n","    filename = os.path.basename(input_path)\n","    name_without_ext = os.path.splitext(filename)[0]\n","\n","    print(f\"\\n{'='*60}\")\n","    print(f\"Processing: {filename}\")\n","    print(f\"{'='*60}\")\n","\n","    # -------------------------\n","    # 1. LOAD + PREPROCESSING\n","    # -------------------------\n","    img = cv2.imread(input_path)\n","    if img is None:\n","        print(f\"ERROR: Could not read image from {input_path}\")\n","        return False\n","\n","    original = img.copy()\n","    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n","    blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n","\n","    # Adaptive threshold\n","    thresh = cv2.adaptiveThreshold(\n","        blurred, 255,\n","        cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n","        cv2.THRESH_BINARY_INV,\n","        11, 2\n","    )\n","\n","    if save_intermediates:\n","        cv2.imwrite(os.path.join(output_folder, f\"{name_without_ext}_01_threshold.jpg\"), thresh)\n","\n","    # -------------------------\n","    # 2. MORPHOLOGICAL OPS\n","    # -------------------------\n","\n","    # Extract horizontal lines\n","    horizontal = thresh.copy()\n","    h_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (40, 1))\n","    horizontal = cv2.erode(horizontal, h_kernel, iterations=1)\n","    horizontal = cv2.dilate(horizontal, h_kernel, iterations=1)\n","\n","    # Extract vertical lines\n","    vertical = thresh.copy()\n","    v_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (1, 40))\n","    vertical = cv2.erode(vertical, v_kernel, iterations=1)\n","    vertical = cv2.dilate(vertical, v_kernel, iterations=1)\n","\n","    # Combine them to get grid mask\n","    grid_mask = cv2.addWeighted(horizontal, 0.5, vertical, 0.5, 0)\n","\n","    if save_intermediates:\n","        cv2.imwrite(os.path.join(output_folder, f\"{name_without_ext}_02_horizontal.jpg\"), horizontal)\n","        cv2.imwrite(os.path.join(output_folder, f\"{name_without_ext}_03_vertical.jpg\"), vertical)\n","        cv2.imwrite(os.path.join(output_folder, f\"{name_without_ext}_04_grid_mask.jpg\"), grid_mask)\n","\n","    # -------------------------\n","    # 3. HOUGH TRANSFORM\n","    # -------------------------\n","\n","    lines = cv2.HoughLinesP(\n","        grid_mask,\n","        rho=1,\n","        theta=np.pi/180,\n","        threshold=150,\n","        minLineLength=100,\n","        maxLineGap=10\n","    )\n","\n","    if lines is None:\n","        print(f\"WARNING: No lines detected for {filename}\")\n","        return False\n","\n","    print(f\"Detected {len(lines)} line segments\")\n","\n","    # ALWAYS save Hough lines visualization\n","    hough_img = original.copy()\n","    for line in lines:\n","        x1, y1, x2, y2 = line[0]\n","        cv2.line(hough_img, (x1, y1), (x2, y2), (0, 255, 0), 2)\n","\n","    hough_output_path = os.path.join(output_folder, f\"{name_without_ext}_hough_lines.jpg\")\n","    cv2.imwrite(hough_output_path, hough_img)\n","    print(f\"✓ Saved Hough lines: {hough_output_path}\")\n","\n","    # -------------------------\n","    # 4. HELPER FUNCTIONS\n","    # -------------------------\n","\n","    def cluster_lines(lines_list, threshold=20):\n","        \"\"\"Cluster similar lines together\"\"\"\n","        if len(lines_list) == 0:\n","            return []\n","\n","        lines_list = sorted(lines_list)\n","        clusters = []\n","        current_cluster = [lines_list[0]]\n","\n","        for line in lines_list[1:]:\n","            if line - current_cluster[-1] < threshold:\n","                current_cluster.append(line)\n","            else:\n","                clusters.append(int(np.mean(current_cluster)))\n","                current_cluster = [line]\n","        clusters.append(int(np.mean(current_cluster)))\n","        return clusters\n","\n","    def find_grid_corners_from_hough(lines, img_shape):\n","        \"\"\"Find the four corners of Sudoku grid from Hough lines\"\"\"\n","        h_lines = []\n","        v_lines = []\n","\n","        for line in lines:\n","            x1, y1, x2, y2 = line[0]\n","            angle = np.abs(np.arctan2(y2 - y1, x2 - x1) * 180 / np.pi)\n","\n","            # Horizontal lines\n","            if angle < 10 or angle > 170:\n","                y_avg = (y1 + y2) // 2\n","                h_lines.append(y_avg)\n","            # Vertical lines\n","            elif 80 < angle < 100:\n","                x_avg = (x1 + x2) // 2\n","                v_lines.append(x_avg)\n","\n","        # Cluster lines to get main grid lines\n","        h_clusters = cluster_lines(h_lines, threshold=20)\n","        v_clusters = cluster_lines(v_lines, threshold=20)\n","\n","        if len(h_clusters) < 2 or len(v_clusters) < 2:\n","            return None\n","\n","        # Get outer boundaries (first and last lines)\n","        top = h_clusters[0]\n","        bottom = h_clusters[-1]\n","        left = v_clusters[0]\n","        right = v_clusters[-1]\n","\n","        # Create corner points\n","        corners = np.array([\n","            [left, top],\n","            [right, top],\n","            [right, bottom],\n","            [left, bottom]\n","        ], dtype=np.float32)\n","\n","        return corners\n","\n","    def order_points(pts):\n","        \"\"\"Order points in clockwise order starting from top-left\"\"\"\n","        rect = np.zeros((4, 2), dtype=\"float32\")\n","\n","        s = pts.sum(axis=1)\n","        rect[0] = pts[np.argmin(s)]\n","        rect[2] = pts[np.argmax(s)]\n","\n","        diff = np.diff(pts, axis=1)\n","        rect[1] = pts[np.argmin(diff)]\n","        rect[3] = pts[np.argmax(diff)]\n","\n","        return rect\n","\n","    def perspective_transform(image, corners):\n","        \"\"\"Apply perspective transform to get top-down view\"\"\"\n","        rect = order_points(corners)\n","        (tl, tr, br, bl) = rect\n","\n","        widthA = np.sqrt(((br[0] - bl[0]) ** 2) + ((br[1] - bl[1]) ** 2))\n","        widthB = np.sqrt(((tr[0] - tl[0]) ** 2) + ((tr[1] - tl[1]) ** 2))\n","        maxWidth = max(int(widthA), int(widthB))\n","\n","        heightA = np.sqrt(((tr[0] - br[0]) ** 2) + ((tr[1] - br[1]) ** 2))\n","        heightB = np.sqrt(((tl[0] - bl[0]) ** 2) + ((tl[1] - bl[1]) ** 2))\n","        maxHeight = max(int(heightA), int(heightB))\n","\n","        size = max(maxWidth, maxHeight)\n","\n","        dst = np.array([\n","            [0, 0],\n","            [size - 1, 0],\n","            [size - 1, size - 1],\n","            [0, size - 1]\n","        ], dtype=\"float32\")\n","\n","        M = cv2.getPerspectiveTransform(rect, dst)\n","        warped = cv2.warpPerspective(image, M, (size, size))\n","\n","        return warped, M\n","\n","    # -------------------------\n","    # 5. FIND CORNERS\n","    # -------------------------\n","\n","    sudoku_corners = find_grid_corners_from_hough(lines, original.shape)\n","\n","    # Fallback: Try contour detection\n","    if sudoku_corners is None:\n","        print(\"Hough method failed, trying contour detection...\")\n","        contours, _ = cv2.findContours(\n","            grid_mask,\n","            cv2.RETR_EXTERNAL,\n","            cv2.CHAIN_APPROX_SIMPLE\n","        )\n","\n","        contours = sorted(contours, key=cv2.contourArea, reverse=True)\n","\n","        for cnt in contours:\n","            perimeter = cv2.arcLength(cnt, True)\n","            approx = cv2.approxPolyDP(cnt, 0.02 * perimeter, True)\n","\n","            if len(approx) == 4:\n","                sudoku_corners = approx.reshape(4, 2).astype(np.float32)\n","                print(\"Using contour corners as fallback\")\n","                break\n","\n","    if sudoku_corners is None:\n","        print(f\"ERROR: Could not detect grid corners for {filename}\")\n","        return False\n","\n","    print(f\"Corners detected: {sudoku_corners.tolist()}\")\n","\n","    # ALWAYS save corners visualization\n","    corners_img = original.copy()\n","    ordered = order_points(sudoku_corners)\n","\n","    # Draw corner points with numbers\n","    for i, corner in enumerate(ordered):\n","        cv2.circle(corners_img, tuple(corner.astype(int)), 15, (0, 0, 255), -1)\n","        cv2.putText(corners_img, str(i), tuple(corner.astype(int)),\n","                   cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 0), 2)\n","\n","    # Draw lines connecting corners (bounding box)\n","    for i in range(4):\n","        pt1 = tuple(ordered[i].astype(int))\n","        pt2 = tuple(ordered[(i+1)%4].astype(int))\n","        cv2.line(corners_img, pt1, pt2, (0, 255, 0), 3)\n","\n","    corners_output_path = os.path.join(output_folder, f\"{name_without_ext}_corners.jpg\")\n","    cv2.imwrite(corners_output_path, corners_img)\n","    print(f\"✓ Saved detected corners: {corners_output_path}\")\n","\n","    # -------------------------\n","    # 6. PERSPECTIVE TRANSFORM\n","    # -------------------------\n","\n","    try:\n","        warped, transform_matrix = perspective_transform(original, sudoku_corners)\n","        warped_gray, _ = perspective_transform(gray, sudoku_corners)\n","        print(f\"✓ Perspective transform successful! Output size: {warped.shape[:2]}\")\n","    except Exception as e:\n","        print(f\"ERROR: Perspective transform failed: {e}\")\n","        return False\n","\n","    # Save other intermediate steps if requested\n","    if save_intermediates:\n","        # Additional intermediate visualizations already saved above\n","        pass\n","\n","    print(f\"✓ Processing complete for {filename}\\n\")\n","    return True\n","\n","\n","def process_sudoku_batch(input_paths, output_folder, save_intermediates=False):\n","    \"\"\"\n","    Process multiple Sudoku images in batch.\n","\n","    Parameters:\n","    -----------\n","    input_paths : list of str\n","        List of input image paths\n","    output_folder : str\n","        Folder to save all processed images\n","    save_intermediates : bool\n","        Whether to save intermediate processing steps\n","\n","    Returns:\n","    --------\n","    dict : Summary of results with success/failure counts\n","    \"\"\"\n","    results = {\n","        'successful': [],\n","        'failed': [],\n","        'total': len(input_paths)\n","    }\n","\n","    for input_path in input_paths:\n","        if not os.path.exists(input_path):\n","            print(f\"WARNING: File not found: {input_path}\")\n","            results['failed'].append(input_path)\n","            continue\n","\n","        success = process_sudoku_image(input_path, output_folder, save_intermediates)\n","\n","        if success:\n","            results['successful'].append(input_path)\n","        else:\n","            results['failed'].append(input_path)\n","\n","    # Print summary\n","    print(f\"\\n{'='*60}\")\n","    print(f\"BATCH PROCESSING COMPLETE\")\n","    print(f\"{'='*60}\")\n","    print(f\"Total images: {results['total']}\")\n","    print(f\"Successful: {len(results['successful'])}\")\n","    print(f\"Failed: {len(results['failed'])}\")\n","\n","    if results['failed']:\n","        print(\"\\nFailed images:\")\n","        for path in results['failed']:\n","            print(f\"  - {os.path.basename(path)}\")\n","\n","    return results\n","\n","\n","# -------------------------\n","# USAGE EXAMPLES\n","# -------------------------\n","\n","if __name__ == \"__main__\":\n","\n","    input_files = [\n","    '/content/drive/MyDrive/Computer Vision/processed_3/01.jpg',\n","    '/content/drive/MyDrive/Computer Vision/processed_3/02.jpg',\n","    '/content/drive/MyDrive/Computer Vision/processed_3/03.jpg',\n","    '/content/drive/MyDrive/Computer Vision/processed_3/04.jpg',\n","    '/content/drive/MyDrive/Computer Vision/processed_3/05.jpg',\n","    '/content/drive/MyDrive/Computer Vision/processed_3/06.jpg',\n","    '/content/drive/MyDrive/Computer Vision/processed_3/07.jpg',\n","    '/content/drive/MyDrive/Computer Vision/processed_3/08.jpg',\n","    '/content/drive/MyDrive/Computer Vision/processed_3/09.jpg',\n","    '/content/drive/MyDrive/Computer Vision/processed_3/10.jpg',\n","    '/content/drive/MyDrive/Computer Vision/processed_3/11.jpg',\n","    '/content/drive/MyDrive/Computer Vision/processed_3/12.jpg',\n","    '/content/drive/MyDrive/Computer Vision/processed_3/13.jpg',\n","    '/content/drive/MyDrive/Computer Vision/processed_3/14.jpg',\n","    '/content/drive/MyDrive/Computer Vision/processed_3/15.jpg',\n","    '/content/drive/MyDrive/Computer Vision/processed_3/16.jpg'\n","]\n","\n","    output_directory = \"/content/drive/MyDrive/Computer Vision/warped_batch/\"\n","\n","    results = process_sudoku_batch(input_files, output_directory, save_intermediates=False)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"GncaQ3VOy9G_"},"source":["Reduce to a squre\n"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"ONNL60RMywEW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1763979073030,"user_tz":-120,"elapsed":1067,"user":{"displayName":"Abdallah Mohamed El Refaey","userId":"11442150962439552777"}},"outputId":"daaba505-27dc-41a1-8ff2-70e407350368"},"outputs":[{"output_type":"stream","name":"stdout","text":["[REUSE] 01.jpg: accepted (areaRatio=0.85, aspect=1.02)\n","[OK] 01.jpg: reuse=True hough_used=False warped=(450, 450)\n","[REUSE] 02.jpg: accepted (areaRatio=0.84, aspect=1.03)\n","[OK] 02.jpg: reuse=True hough_used=False warped=(450, 450)\n","[REUSE] 03.jpg: accepted (areaRatio=0.86, aspect=0.99)\n","[OK] 03.jpg: reuse=True hough_used=False warped=(450, 450)\n","[REUSE] 04.jpg: accepted (areaRatio=0.87, aspect=1.00)\n","[OK] 04.jpg: reuse=True hough_used=False warped=(450, 450)\n","[REUSE] 05.jpg: accepted (areaRatio=0.77, aspect=1.01)\n","[OK] 05.jpg: reuse=True hough_used=False warped=(450, 450)\n","[REUSE] 06.jpg: accepted (areaRatio=0.75, aspect=1.04)\n","[OK] 06.jpg: reuse=True hough_used=False warped=(450, 450)\n","[REUSE] 07.jpg: accepted (areaRatio=0.80, aspect=0.96)\n","[OK] 07.jpg: reuse=True hough_used=False warped=(450, 450)\n","[REUSE] 08.jpg: accepted (areaRatio=0.84, aspect=0.98)\n","[OK] 08.jpg: reuse=True hough_used=False warped=(450, 450)\n","[REUSE] 09.jpg: accepted (areaRatio=0.84, aspect=1.03)\n","[OK] 09.jpg: reuse=True hough_used=False warped=(450, 450)\n","[REUSE] 10.jpg: accepted (areaRatio=0.85, aspect=1.03)\n","[OK] 10.jpg: reuse=True hough_used=False warped=(450, 450)\n","[REUSE] 11.jpg: accepted (areaRatio=0.92, aspect=1.00)\n","[OK] 11.jpg: reuse=True hough_used=False warped=(450, 450)\n","[REUSE] 12.jpg: accepted (areaRatio=0.54, aspect=1.01)\n","[OK] 12.jpg: reuse=True hough_used=False warped=(450, 450)\n","[REUSE] 13.jpg: accepted (areaRatio=0.87, aspect=1.01)\n","[OK] 13.jpg: reuse=True hough_used=False warped=(450, 450)\n","[REUSE] 14.jpg: accepted (areaRatio=0.83, aspect=1.00)\n","[OK] 14.jpg: reuse=True hough_used=False warped=(450, 450)\n","[REUSE] 15.jpg: accepted (areaRatio=0.89, aspect=0.98)\n","[OK] 15.jpg: reuse=True hough_used=False warped=(450, 450)\n","[REUSE] 16.jpg: accepted (areaRatio=0.58, aspect=1.00)\n","[OK] 16.jpg: reuse=True hough_used=False warped=(450, 450)\n","\n","Batch refinement complete: success=16, failed=0, total=16\n"]}],"source":["\n","import cv2\n","import numpy as np\n","import os\n","from pathlib import Path\n","\n","REFINE_INPUT_DIR = '/content/drive/MyDrive/Computer Vision/processed_3'\n","REFINE_OUTPUT_DIR = '/content/drive/MyDrive/Computer Vision/final_square'\n","PREVIOUS_OUTPUT_DIR = '/content/drive/MyDrive/Computer Vision/warped_batch'\n","os.makedirs(REFINE_OUTPUT_DIR, exist_ok=True)\n","\n","TARGET_SIZE = 450\n","MIN_LINE_LENGTH_FACTOR = 0.4\n","GRID_EXPECTED_LINES = 10\n","ROTATION_MIN_CORRECTION = 2.0\n","ROTATION_MAX_CORRECTION = 12.0\n","VERTICAL_MIN_COUNT = 6\n","VERTICAL_STD_MAX = 2.5\n","UNIFORMITY_RATIO_LIMIT = 2.2  # slightly stricter than spacing_uniform fallback\n","\n","# -----------------------------\n","# DETECT GRID LINES (morphology + Hough)\n","# -----------------------------\n","def detect_grid_lines(gray):\n","    blur = cv2.GaussianBlur(gray, (5, 5), 0)\n","    thresh = cv2.adaptiveThreshold(blur, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY_INV, 11, 2)\n","    h = thresh.copy(); v = thresh.copy()\n","    h_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (40, 1))\n","    v_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (1, 40))\n","    h = cv2.erode(h, h_kernel, iterations=1); h = cv2.dilate(h, h_kernel, iterations=1)\n","    v = cv2.erode(v, v_kernel, iterations=1); v = cv2.dilate(v, v_kernel, iterations=1)\n","    grid_mask = cv2.addWeighted(h, 0.5, v, 0.5, 0)\n","    h_val, w_val = gray.shape\n","    min_line_len = int(min(h_val, w_val) * MIN_LINE_LENGTH_FACTOR)\n","    lines = cv2.HoughLinesP(grid_mask, 1, np.pi/180, 120, minLineLength=min_line_len, maxLineGap=15)\n","    if lines is None:\n","        lines = cv2.HoughLinesP(thresh, 1, np.pi/180, 150, minLineLength=min_line_len, maxLineGap=20)\n","    return lines, grid_mask, thresh\n","\n","# -----------------------------\n","# CLASSIFY LINES\n","# -----------------------------\n","def classify_lines(lines):\n","    horizontal_positions, vertical_positions = [], []\n","    if lines is None: return horizontal_positions, vertical_positions, []\n","    vertical_angles = []\n","    for seg in lines:\n","        x1,y1,x2,y2 = seg[0]\n","        dx, dy = x2 - x1, y2 - y1\n","        ang = np.degrees(np.arctan2(dy, dx))\n","        abs_ang = abs(ang)\n","        if abs_ang < 10 or abs_ang > 170:\n","            horizontal_positions.append((y1 + y2)//2)\n","        elif 80 < abs_ang < 100:\n","            vertical_positions.append((x1 + x2)//2)\n","            if ang < 0: ang += 180\n","            vertical_angles.append(ang)\n","    return horizontal_positions, vertical_positions, vertical_angles\n","\n","# -----------------------------\n","# ROTATION ESTIMATION\n","# -----------------------------\n","def estimate_rotation(vertical_angles):\n","    if len(vertical_angles) < VERTICAL_MIN_COUNT:\n","        return 0.0, False\n","    arr = np.array(vertical_angles)\n","    mean_ang = arr.mean(); std_ang = arr.std()\n","    correction = mean_ang - 90.0\n","    reliable = (std_ang < VERTICAL_STD_MAX and ROTATION_MIN_CORRECTION < abs(correction) < ROTATION_MAX_CORRECTION)\n","    return correction, reliable\n","\n","# -----------------------------\n","# APPLY ROTATION\n","# -----------------------------\n","def apply_rotation(image, angle):\n","    h, w = image.shape[:2]\n","    M = cv2.getRotationMatrix2D((w/2, h/2), angle, 1.0)\n","    return cv2.warpAffine(image, M, (w, h), flags=cv2.INTER_LINEAR, borderMode=cv2.BORDER_REPLICATE)\n","\n","# -----------------------------\n","# CLUSTER POSITIONS\n","# -----------------------------\n","def cluster_positions(positions, threshold):\n","    if not positions: return []\n","    positions = sorted(positions)\n","    clusters, current = [], [positions[0]]\n","    for p in positions[1:]:\n","        if p - current[-1] <= threshold:\n","            current.append(p)\n","        else:\n","            clusters.append(int(np.mean(current)))\n","            current = [p]\n","    clusters.append(int(np.mean(current)))\n","    return clusters\n","\n","# -----------------------------\n","# SELECT BEST 10 UNIFORMLY SPACED LINES (avoid margins)\n","# -----------------------------\n","def select_best_uniform_10(sorted_positions):\n","    # Requires >=10 positions\n","    if len(sorted_positions) < GRID_EXPECTED_LINES:\n","        return None\n","    best = None\n","    best_score = 1e9\n","    for i in range(len(sorted_positions) - GRID_EXPECTED_LINES + 1):\n","        window = sorted_positions[i:i+GRID_EXPECTED_LINES]\n","        diffs = np.diff(window)\n","        if len(diffs) == 0: continue\n","        max_d = np.max(diffs); min_d = np.min(diffs)\n","        if min_d == 0: continue\n","        ratio = max_d / min_d\n","        # Score: combination of ratio and std for stability\n","        std = np.std(diffs)\n","        score = ratio * (1 + std / (np.mean(diffs)+1e-6))\n","        if score < best_score:\n","            best_score = score\n","            best = window\n","    # Sanity: if chosen ratio still poor, return None\n","    if best is not None:\n","        diffs = np.diff(best)\n","        max_d = np.max(diffs); min_d = np.min(diffs)\n","        if min_d == 0 or (max_d / min_d) > UNIFORMITY_RATIO_LIMIT:\n","            return None\n","    return best\n","\n","# -----------------------------\n","# COMPLETE LINES TO EXACT 10\n","# -----------------------------\n","def complete_lines(line_positions, image_extent):\n","    unique = sorted(set(line_positions))\n","    if len(unique) == GRID_EXPECTED_LINES: return unique\n","    if len(unique) < 2:\n","        return [int(round(i * image_extent / (GRID_EXPECTED_LINES - 1))) for i in range(GRID_EXPECTED_LINES)]\n","    first, last = unique[0], unique[-1]\n","    span = last - first\n","    if span < image_extent * 0.5:\n","        first, last = 0, image_extent - 1\n","        span = last - first\n","    step = span / (GRID_EXPECTED_LINES - 1)\n","    return [int(round(first + i * step)) for i in range(GRID_EXPECTED_LINES)]\n","\n","# -----------------------------\n","# SPACING VALIDATION\n","# -----------------------------\n","def spacing_uniform(line_positions):\n","    diffs = np.diff(sorted(line_positions))\n","    if len(diffs) == 0: return False\n","    max_d, min_d = np.max(diffs), np.min(diffs)\n","    if min_d == 0: return False\n","    return (max_d / min_d) < 2.5\n","\n","# -----------------------------\n","# Fallbacks: contour & bounding box\n","# -----------------------------\n","def contour_outer_corners(thresh):\n","    cnts = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n","    cnts = cnts[0] if len(cnts) == 2 else cnts[1]\n","    cnts = sorted(cnts, key=cv2.contourArea, reverse=True)\n","    for c in cnts:\n","        peri = cv2.arcLength(c, True)\n","        approx = cv2.approxPolyDP(c, 0.02 * peri, True)\n","        if len(approx) == 4:\n","            return approx.reshape(4,2).astype(np.float32)\n","    return None\n","\n","\n","def bounding_box_corners(mask):\n","    ys, xs = np.where(mask > 0)\n","    if len(xs) == 0 or len(ys) == 0: return None\n","    left, right = int(xs.min()), int(xs.max())\n","    top, bottom = int(ys.min()), int(ys.max())\n","    return np.array([[left, top],[right, top],[right, bottom],[left, bottom]], dtype=np.float32)\n","\n","# -----------------------------\n","# DERIVE CORNERS FROM COMPLETED LINES\n","# -----------------------------\n","def derive_corners(h_lines, v_lines):\n","    return np.array([[v_lines[0], h_lines[0]],[v_lines[-1], h_lines[0]],[v_lines[-1], h_lines[-1]],[v_lines[0], h_lines[-1]]], dtype=np.float32)\n","\n","# -----------------------------\n","# ORDER POINTS & WARP TO SQUARE\n","# -----------------------------\n","def order_points(pts):\n","    rect = np.zeros((4, 2), dtype=\"float32\")\n","    s = pts.sum(axis=1)\n","    rect[0] = pts[np.argmin(s)]\n","    rect[2] = pts[np.argmax(s)]\n","    diff = np.diff(pts, axis=1)\n","    rect[1] = pts[np.argmin(diff)]\n","    rect[3] = pts[np.argmax(diff)]\n","    return rect\n","\n","\n","def warp_to_square(image, corners, size=TARGET_SIZE):\n","    rect = order_points(corners)\n","    dst = np.array([[0,0],[size-1,0],[size-1,size-1],[0,size-1]], dtype=np.float32)\n","    M = cv2.getPerspectiveTransform(rect, dst)\n","    warped = cv2.warpPerspective(image, M, (size, size))\n","    return warped, M\n","\n","# -----------------------------\n","# ENHANCE WARPED IMAGE\n","# -----------------------------\n","def enhance_warped(warped_gray):\n","    clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8,8))\n","    eq = clahe.apply(warped_gray)\n","    edges = cv2.Canny(eq, 40, 120)\n","    edges = cv2.dilate(edges, np.ones((2,2), np.uint8), iterations=1)\n","    reinforced = cv2.addWeighted(eq, 1.0, edges, 0.15, 0)\n","    blur = cv2.GaussianBlur(reinforced, (3,3), 0)\n","    sharpened = cv2.addWeighted(reinforced, 1.25, blur, -0.25, 0)\n","    return sharpened\n","\n","# -----------------------------\n","# EXTRACT CORNERS FROM PREVIOUS CORNERS IMAGE (RED CIRCLES)\n","# -----------------------------\n","def extract_corners_from_previous(stem):\n","    corners_img_path = os.path.join(PREVIOUS_OUTPUT_DIR, f\"{stem}_corners.jpg\")\n","    if not os.path.exists(corners_img_path): return None\n","    img = cv2.imread(corners_img_path)\n","    if img is None: return None\n","    hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n","    lower1 = np.array([0, 70, 50]); upper1 = np.array([10, 255, 255])\n","    lower2 = np.array([160, 70, 50]); upper2 = np.array([180, 255, 255])\n","    mask = cv2.bitwise_or(cv2.inRange(hsv, lower1, upper1), cv2.inRange(hsv, lower2, upper2))\n","    mask = cv2.medianBlur(mask, 5)\n","    cnts = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n","    cnts = cnts[0] if len(cnts) == 2 else cnts[1]\n","    centers = []\n","    for c in cnts:\n","        area = cv2.contourArea(c)\n","        if 150 < area < 1500:\n","            M = cv2.moments(c)\n","            if M['m00'] > 0:\n","                centers.append([int(M['m10']/M['m00']), int(M['m01']/M['m00'])])\n","    if len(centers) != 4: return None\n","    return np.array(centers, dtype=np.float32)\n","\n","# -----------------------------\n","# PARSE HOUGH OVERLAY (GREEN LINES)\n","# -----------------------------\n","def extract_lines_from_hough_overlay(stem):\n","    path = os.path.join(PREVIOUS_OUTPUT_DIR, f\"{stem}_hough_lines.jpg\")\n","    if not os.path.exists(path): return None\n","    img = cv2.imread(path)\n","    if img is None: return None\n","    hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n","    lower = np.array([35, 60, 40]); upper = np.array([85, 255, 255])\n","    mask = cv2.inRange(hsv, lower, upper)\n","    mask = cv2.dilate(mask, np.ones((3,3), np.uint8), iterations=1)\n","    lines = cv2.HoughLinesP(mask, 1, np.pi/180, 60, minLineLength=40, maxLineGap=15)\n","    return lines\n","\n","# -----------------------------\n","# HOUGH OVERLAY CORNERS (with LS outer fits)\n","# -----------------------------\n","def corners_from_hough_overlay(lines, img_shape):\n","    if lines is None or len(lines) < 4: return None\n","    h_pos = []; v_pos = []; h_segments = []; v_segments = []\n","    for seg in lines:\n","        x1,y1,x2,y2 = seg[0]; dx, dy = x2 - x1, y2 - y1\n","        ang = abs(np.degrees(np.arctan2(dy, dx)))\n","        if ang < 12 or ang > 168:\n","            h_pos.append((y1+y2)/2.0); h_segments.append([(x1,y1),(x2,y2)])\n","        elif 78 < ang < 102:\n","            v_pos.append((x1+x2)/2.0); v_segments.append([(x1,y1),(x2,y2)])\n","    if len(h_pos) < 2 or len(v_pos) < 2: return None\n","    h_thresh = max(10, img_shape[0]//80); v_thresh = max(10, img_shape[1]//80)\n","    h_clusters = cluster_positions(h_pos, h_thresh)\n","    v_clusters = cluster_positions(v_pos, v_thresh)\n","    # Inner selection if many clusters\n","    if len(h_clusters) > GRID_EXPECTED_LINES:\n","        sel = select_best_uniform_10(h_clusters)\n","        if sel is not None: h_clusters = sel\n","    if len(v_clusters) > GRID_EXPECTED_LINES:\n","        sel = select_best_uniform_10(v_clusters)\n","        if sel is not None: v_clusters = sel\n","    if len(h_clusters) < 2 or len(v_clusters) < 2: return None\n","    top_y, bottom_y = h_clusters[0], h_clusters[-1]\n","    left_x, right_x = v_clusters[0], v_clusters[-1]\n","    def collect(seg_list, target, axis='y', tol=15):\n","        pts = []\n","        for seg in seg_list:\n","            for (x,y) in seg:\n","                val = y if axis=='y' else x\n","                if abs(val - target) <= tol: pts.append((x,y))\n","        return pts\n","    top_pts = collect(h_segments, top_y, 'y'); bottom_pts = collect(h_segments, bottom_y, 'y')\n","    left_pts = collect(v_segments, left_x, 'x'); right_pts = collect(v_segments, right_x, 'x')\n","    def fit_line(points, mode='horizontal'):\n","        if len(points) < 2: return None\n","        pts = np.array(points, dtype=np.float32); x = pts[:,0]; y = pts[:,1]\n","        if mode=='horizontal':\n","            A = np.vstack([x, np.ones_like(x)]).T; m, b = np.linalg.lstsq(A, y, rcond=None)[0]; return ('h', m, b)\n","        else:\n","            A = np.vstack([y, np.ones_like(y)]).T; m, b = np.linalg.lstsq(A, x, rcond=None)[0]; return ('v', m, b)\n","    top_line = fit_line(top_pts, 'horizontal'); bottom_line = fit_line(bottom_pts, 'horizontal')\n","    left_line = fit_line(left_pts, 'vertical'); right_line = fit_line(right_pts, 'vertical')\n","    if None in (top_line,bottom_line,left_line,right_line): return None\n","    def line_to_abc(line):\n","        kind,m,b = line\n","        if kind=='h': return m, -1.0, b\n","        return 1.0, -m, -b\n","    def intersect(l1,l2):\n","        a1,b1,c1 = line_to_abc(l1); a2,b2,c2 = line_to_abc(l2); det = a1*b2 - a2*b1\n","        if abs(det)<1e-8: return None\n","        x = (-c1*b2 + c2*b1)/det; y = (-a1*c2 + a2*c1)/det\n","        return np.array([x,y], dtype=np.float32)\n","    tl = intersect(top_line,left_line); tr = intersect(top_line,right_line)\n","    br = intersect(bottom_line,right_line); bl = intersect(bottom_line,left_line)\n","    if None in (tl,tr,br,bl): return None\n","    corners = np.array([tl,tr,br,bl], dtype=np.float32)\n","    xs = corners[:,0]; ys = corners[:,1]\n","    if not (0 <= xs.min() < img_shape[1] and 0 <= xs.max() <= img_shape[1] and 0 <= ys.min() < img_shape[0] and 0 <= ys.max() <= img_shape[0]):\n","        return None\n","    return corners\n","\n","# -----------------------------\n","# MAIN REFINEMENT (INCLUDES INNER SELECTION)\n","# -----------------------------\n","def refine_sudoku_image(input_path, output_dir):\n","    base_name = os.path.basename(input_path); stem = os.path.splitext(base_name)[0]\n","    img = cv2.imread(input_path)\n","    if img is None:\n","        print(f\"[FAIL] Cannot read {input_path}\"); return False\n","    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n","    reuse_success = False; hough_success = False\n","    # 1. Reuse corners (red circles)\n","    reused_corners = extract_corners_from_previous(stem)\n","    if reused_corners is not None:\n","        xs = reused_corners[:,0]; ys = reused_corners[:,1]\n","        w_span = xs.max()-xs.min(); h_span = ys.max()-ys.min()\n","        area_ratio = (w_span*h_span)/(gray.shape[0]*gray.shape[1]); aspect = w_span/max(1,h_span)\n","        if area_ratio > 0.25 and 0.6 < aspect < 1.6:\n","            corners = reused_corners; reuse_success = True\n","            print(f\"[REUSE] {base_name}: accepted (areaRatio={area_ratio:.2f}, aspect={aspect:.2f})\")\n","        else:\n","            print(f\"[REUSE-REJECT] {base_name}: sanity failed\")\n","    # 2. Hough overlay parse\n","    if not reuse_success:\n","        hough_lines = extract_lines_from_hough_overlay(stem)\n","        if hough_lines is not None:\n","            overlay_corners = corners_from_hough_overlay(hough_lines, gray.shape)\n","            if overlay_corners is not None:\n","                xs = overlay_corners[:,0]; ys = overlay_corners[:,1]\n","                w_span = xs.max()-xs.min(); h_span = ys.max()-ys.min()\n","                area_ratio = (w_span*h_span)/(gray.shape[0]*gray.shape[1]); aspect = w_span/max(1,h_span)\n","                if area_ratio > 0.25 and 0.6 < aspect < 1.6:\n","                    corners = overlay_corners; hough_success = True\n","                    print(f\"[HOUGH-OVERLAY] {base_name}: accepted (areaRatio={area_ratio:.2f}, aspect={aspect:.2f})\")\n","                else:\n","                    print(f\"[HOUGH-REJECT] {base_name}: sanity failed\")\n","            else:\n","                print(f\"[HOUGH-NO-CORNERS] {base_name}\")\n","        else:\n","            print(f\"[HOUGH-NO-LINES] {base_name}\")\n","    # 3. Fallback detection path\n","    if not (reuse_success or hough_success):\n","        lines_pass1, grid_mask1, thresh1 = detect_grid_lines(gray)\n","        h_pos1, v_pos1, v_angles1 = classify_lines(lines_pass1)\n","        correction, reliable = estimate_rotation(v_angles1)\n","        if reliable:\n","            img = apply_rotation(img, -correction); gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n","            print(f\"[ROTATE] {base_name}: {-correction:.2f}° applied\")\n","            lines, grid_mask, thresh = detect_grid_lines(gray)\n","        else:\n","            lines, grid_mask, thresh = lines_pass1, grid_mask1, thresh1\n","            print(f\"[NO-ROTATE] {base_name}: correction={correction:.2f} reliable={reliable} count={len(v_angles1)}\")\n","        horiz, vert, _ = classify_lines(lines)\n","        h_thresh = max(10, gray.shape[0]//80); v_thresh = max(10, gray.shape[1]//80)\n","        h_clusters = cluster_positions(horiz, h_thresh); v_clusters = cluster_positions(vert, v_thresh)\n","        # Inner selection before completion\n","        if len(h_clusters) > GRID_EXPECTED_LINES:\n","            sel = select_best_uniform_10(h_clusters)\n","            if sel is not None: h_clusters = sel\n","        if len(v_clusters) > GRID_EXPECTED_LINES:\n","            sel = select_best_uniform_10(v_clusters)\n","            if sel is not None: v_clusters = sel\n","        h_completed = h_clusters if len(h_clusters)==GRID_EXPECTED_LINES else complete_lines(h_clusters, gray.shape[0])\n","        v_completed = v_clusters if len(v_clusters)==GRID_EXPECTED_LINES else complete_lines(v_clusters, gray.shape[1])\n","        spacing_ok = spacing_uniform(h_completed) and spacing_uniform(v_completed)\n","        corners = derive_corners(h_completed, v_completed)\n","        area_lines = (max(v_completed)-min(v_completed))*(max(h_completed)-min(h_completed))\n","        full_area = gray.shape[0]*gray.shape[1]; area_ratio = area_lines/full_area\n","        aspect = (max(v_completed)-min(v_completed))/max(1,(max(h_completed)-min(h_completed)))\n","        unreliable = (area_ratio < 0.35) or (aspect < 0.75 or aspect > 1.35) or (not spacing_ok)\n","        if unreliable:\n","            contour_c = contour_outer_corners(thresh)\n","            if contour_c is not None:\n","                corners = contour_c; print(f\"[FALLBACK-CONTOUR] {base_name}\")\n","            else:\n","                bbox_c = bounding_box_corners(grid_mask)\n","                if bbox_c is not None:\n","                    corners = bbox_c; print(f\"[FALLBACK-BOX] {base_name}\")\n","                else:\n","                    print(f\"[FAIL] {base_name}: no reliable corners\"); return False\n","        else:\n","            print(f\"[LINES] {base_name}: accepted (areaRatio={area_ratio:.2f}, aspect={aspect:.2f}, spacing_ok={spacing_ok})\")\n","    # Warp\n","    warped_color, M = warp_to_square(img, corners, size=TARGET_SIZE)\n","    warped_gray = cv2.cvtColor(warped_color, cv2.COLOR_BGR2GRAY)\n","    enhanced = enhance_warped(warped_gray)\n","    # Visualization\n","    grid_vis = img.copy()\n","    if 'h_completed' in locals() and not (reuse_success or hough_success):\n","        for y in h_completed: cv2.line(grid_vis, (0,y), (grid_vis.shape[1]-1,y), (0,255,0), 2)\n","        for x in v_completed: cv2.line(grid_vis, (x,0), (grid_vis.shape[0]-1,x), (255,0,0), 2)\n","    for c in corners: cv2.circle(grid_vis, tuple(c.astype(int)), 10, (0,0,255), -1)\n","    cv2.imwrite(os.path.join(output_dir, f\"{stem}_square_gray.jpg\"), warped_gray)\n","    print(f\"[OK] {base_name}: reuse={reuse_success} hough_used={hough_success} warped={warped_color.shape[:2]}\")\n","    return True\n","\n","# -----------------------------\n","# BATCH REFINEMENT\n","# -----------------------------\n","def refine_batch(input_dir, output_dir):\n","    paths = [p for p in Path(input_dir).glob('*.jpg')]\n","    success = fail = 0\n","    for p in paths:\n","        if refine_sudoku_image(str(p), output_dir): success += 1\n","        else: fail += 1\n","    print(f\"\\nBatch refinement complete: success={success}, failed={fail}, total={len(paths)}\")\n","\n","if __name__ == '__main__':\n","    refine_batch(REFINE_INPUT_DIR, REFINE_OUTPUT_DIR)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q-AbYEdZUzQ1"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.4"}},"nbformat":4,"nbformat_minor":0}